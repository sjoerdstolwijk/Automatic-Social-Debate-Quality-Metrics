{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afa41b1-eafc-4df0-a7e5-164d287544fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import (make_pipeline, Pipeline)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, GridSearchCV)\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfad24f-c9ca-42f2-84ff-00a66fb4d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variables = ['INTERACTIVITY_DUMMY','INCIVILITY_DUMMY','HATELIST_FOCUSED_DUMMY',\n",
    "                 'RATIONALITY_DUMMY','HAS_OPINION_DUMMY','LIBERAL_DUMMY','CONSERVATIVE_DUMMY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161de334-0f83-4ac6-9e4c-06c7f3f3c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_majority(df, majortopic):\n",
    "        majority = int(len(df[df[majortopic]==0])/len(df)<0.5) # when the ratio of label=0 < .5, majority = 1, else majority = 0\n",
    "        monority = 1 - majority # if majority = 1 then minority = 0, and vice versa\n",
    "        df_majority = df[df[majortopic]==majority]\n",
    "        df_minority = df[df[majortopic]==monority]\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                         replace=False,     #\n",
    "                                         n_samples=len(df[df[majortopic]==1]), # set to N of minority topic\n",
    "                                         random_state=123) #\n",
    "\n",
    "        df_downsampled = pd.concat([df_minority, df_majority_downsampled])\n",
    "        return df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebed0e2-06eb-4601-8b2f-4f5b95de1fe4",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7b355b-b506-47c3-8f61-cdcc89c4b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_configuration():\n",
    "    Vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "    Classifiers = [MultinomialNB(), LogisticRegression(max_iter=1000),\n",
    "                   SVC(kernel='rbf', class_weight=\"balanced\"), SVC(kernel='linear', class_weight=\"balanced\")\n",
    "                  ]\n",
    "    config = [Vectorizers, Classifiers]\n",
    "    configurations = list(itertools.product(*config))\n",
    "    return configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b4f97e-c934-49bf-a9ea-396a30662c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning(train, test, labels):\n",
    "    acc = pd.DataFrame(columns = ['Vectorizer', 'Classifier','Parameters', 'F1_score','Recall','Precision','Accuracy','Ratio_resampled'])\n",
    "\n",
    "    df_downsampled = down_sample_majority(train, labels)\n",
    "    train_labels = df_downsampled[labels]\n",
    "    train_texts = df_downsampled['commentText']\n",
    "    test_labels = test[labels]\n",
    "    test_texts = test['commentText']\n",
    "#    train_texts, test_texts, train_labels, test_labels = train_test_split(df_downsampled['commentText'].to_list(), df_downsampled[labels].to_list(), test_size=0.2, random_state=42)\n",
    "#    print(f'after undersampling:\\ntrain: {len(train_labels)}, test: {len(test_labels)}')\n",
    "#    print(Counter(train_labels))\n",
    "\n",
    "    configurations = combine_configuration()\n",
    "    \n",
    "    for vectorizer, classifier in configurations:\n",
    "        pipeline = Pipeline(steps = [\n",
    "          (\"vectorizer\", vectorizer()), \n",
    "          (\"classifier\", classifier)])\n",
    "\n",
    "        grid = {\"vectorizer__ngram_range\": [(1,1), (1,2)],\n",
    "                \"vectorizer__max_df\": [0.5, 1.0],\n",
    "                \"vectorizer__min_df\": [0, 5],\n",
    "                \"classifier__C\": [0.01, 1, 100]\n",
    "               }\n",
    "        \n",
    "        try:\n",
    "            search=GridSearchCV(estimator=pipeline, n_jobs=-1, param_grid=grid, scoring='f1', cv=5)\n",
    "            search.fit(train_texts, train_labels)\n",
    "        except:\n",
    "            #print('regularization is not applicable')\n",
    "            grid.pop('classifier__C')\n",
    "            search=GridSearchCV(estimator=pipeline, n_jobs=-1, param_grid=grid, scoring='f1', cv=5)\n",
    "            search.fit(train_texts, train_labels)\n",
    "        #print(search.cv_results_['split1_test_score'])\n",
    "        y_pred = search.predict(test_texts)\n",
    "        acc = acc.append({'Vectorizer':vectorizer, 'Classifier':classifier,'Parameters':search.best_params_, \n",
    "                          'F1_score':metrics.f1_score(test_labels, y_pred),'Recall':metrics.recall_score(test_labels, y_pred),\n",
    "                          'Precision':metrics.precision_score(test_labels, y_pred),'Accuracy':metrics.accuracy_score(test_labels, y_pred),\n",
    "                          'Ratio_resampled':Counter(train_labels)[1]/(len(train_labels)),'Manual':test_labels,'Prediction':y_pred,\n",
    "                          'split0_test_score':search.cv_results_['split0_test_score'].mean(),'split1_test_score':search.cv_results_['split1_test_score'].mean(),\n",
    "                          'split2_test_score':search.cv_results_['split2_test_score'].mean(),'split3_test_score':search.cv_results_['split3_test_score'].mean(),\n",
    "                          'split4_test_score':search.cv_results_['split4_test_score'].mean(),\n",
    "                          'split0_test_score_std':search.cv_results_['split0_test_score'].std(),'split1_test_score_std':search.cv_results_['split1_test_score'].std(),\n",
    "                          'split2_test_score_std':search.cv_results_['split2_test_score'].std(),'split3_test_score_std':search.cv_results_['split3_test_score'].std(),\n",
    "                          'split4_test_score_std':search.cv_results_['split4_test_score'].std()},ignore_index=True)\n",
    "    best_classifier = acc[acc['F1_score'] == acc['F1_score'].max()].reset_index()\n",
    "    #print('algorithm with maximum F1_score:', best_classifier)\n",
    "    return acc, best_classifier['Prediction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5321d-fc9d-4324-aca3-da33d44cc417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERACTIVITY_DUMMY\n",
      "INCIVILITY_DUMMY\n",
      "HATELIST_FOCUSED_DUMMY\n",
      "RATIONALITY_DUMMY\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv('data/train.csv')[test_variables+['ID']+['commentText']]  \n",
    "test_set = pd.read_csv('data/test.csv')[test_variables+['ID']+['commentText']]  \n",
    "accuracy = pd.DataFrame(columns = ['Variable', 'Vectorizer', 'Classifier','Parameters', 'F1_score','Recall','Precision','Accuracy','Ratio_test','Ratio_resampled','Manual','Prediction'])\n",
    "\n",
    "for v in test_variables:\n",
    "    print(v)\n",
    "    acc,prediction = machine_learning(train_set, test_set, v)\n",
    "    test_set[v+'_ML'] = prediction\n",
    "\n",
    "    acc['Variable'] = v\n",
    "    acc['Ratio_test'] = test_set[v].mean()\n",
    "    acc['Ratio_prediction'] = prediction.mean()\n",
    "    accuracy = accuracy.append(acc,ignore_index=True)\n",
    "accuracy.to_csv(f'outputs/evaluation/ML_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c4b0e-c941-4876-8ff8-e91cef81c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.DataFrame()\n",
    "for v in test_variables:\n",
    "    select_var = accuracy[accuracy['Variable'] == v]\n",
    "    best_models = best_models.append(select_var[select_var['F1_score'] == select_var['F1_score'].max()],ignore_index=True)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cf449-08e9-429c-b28f-361756ac4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('outputs/automated_results/prediction_ML.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089cbaf6-cd1e-4682-95a5-c8b89cafe124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
